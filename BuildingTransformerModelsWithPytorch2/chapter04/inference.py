from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

def get_device():
    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = 'mps'
    else:
        device = "cpu"
    return device

def inference(text, model_path='saved_model/best_model', label=None, device=None):
    # è·å–è®¾å¤‡
    if device is None:
        device = get_device()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    print(f"æ­£åœ¨ä» {model_path} åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    # Move input tensors to the specified device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Set the model to evaluation mode and perform inference
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Get the predicted probabilities
    probabilities = torch.softmax(logits, dim=1)
    pred_label_idx = torch.argmax(logits, dim=1).item()
    confidence = probabilities[0][pred_label_idx].item()
    
    # Map label index to meaningful text
    label_map = {0: "å‡æ–°é—»", 1: "çœŸæ–°é—»"}
    predicted_label = label_map.get(pred_label_idx, f"æœªçŸ¥æ ‡ç­¾ {pred_label_idx}")
    
    print(f"ğŸ“° æ–‡æœ¬: {text[:100]}...")
    print(f"ğŸ”® é¢„æµ‹ç»“æœ: {predicted_label} (ç´¢å¼•: {pred_label_idx})")
    print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.4f}")
    
    if label is not None:
        actual_label = label_map.get(label, f"æœªçŸ¥æ ‡ç­¾ {label}")
        print(f"âœ… å®é™…æ ‡ç­¾: {actual_label}")
        print(f"ğŸ¯ é¢„æµ‹{'æ­£ç¡®' if pred_label_idx == label else 'é”™è¯¯'}")
    
    print("-" * 50)
    return pred_label_idx, confidence

# æ‰¹é‡æ¨ç†å‡½æ•°
def batch_inference(texts, model_path='saved_model/best_model', labels=None, device=None):
    if device is None:
        device = get_device()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    print(f"æ­£åœ¨ä» {model_path} åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    
    # Tokenize all texts
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Perform batch inference
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Get predictions
    probabilities = torch.softmax(logits, dim=1)
    pred_label_indices = torch.argmax(logits, dim=1).cpu().numpy()
    confidences = probabilities.max(dim=1).values.cpu().numpy()
    
    # Map label indices to meaningful text
    label_map = {0: "å‡æ–°é—»", 1: "çœŸæ–°é—»"}
    
    print("ğŸ“Š æ‰¹é‡æ¨ç†ç»“æœ:")
    print("=" * 60)
    for i, (text, pred_idx, conf) in enumerate(zip(texts, pred_label_indices, confidences)):
        predicted_label = label_map.get(pred_idx, f"æœªçŸ¥æ ‡ç­¾ {pred_idx}")
        print(f"{i+1}. é¢„æµ‹: {predicted_label} | ç½®ä¿¡åº¦: {conf:.4f}")
        print(f"   æ–‡æœ¬: {text[:80]}...")
        if labels is not None:
            actual_label = label_map.get(labels[i], f"æœªçŸ¥æ ‡ç­¾ {labels[i]}")
            correct = "âœ…" if pred_idx == labels[i] else "âŒ"
            print(f"   å®é™…: {actual_label} {correct}")
        print("-" * 40)
    
    return pred_label_indices, confidences

# æµ‹è¯•å‡½æ•°
def test_saved_model():
    """æµ‹è¯•ä¿å­˜çš„æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ§ª æµ‹è¯•ä¿å­˜çš„æ¨¡å‹...")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Scientists have discovered a new breakthrough in renewable energy technology that could revolutionize the industry.",
        "BREAKING: Celebrities are hiding the secret to eternal youth from the public! You won't believe what they know!",
        "The government announced new economic policies today that aim to stimulate growth and create jobs.",
        "SHOCKING: Government cover-up of alien contact revealed by anonymous sources!"
    ]
    
    test_labels = [1, 0, 1, 0]  # 1=çœŸæ–°é—», 0=å‡æ–°é—»
    
    # æµ‹è¯•å•ä¸ªæ¨ç†
    print("å•ä¸ªæ¨ç†æµ‹è¯•:")
    for i, text in enumerate(test_texts[:2]):
        inference(text, label=test_labels[i])
    
    # æµ‹è¯•æ‰¹é‡æ¨ç†
    print("\næ‰¹é‡æ¨ç†æµ‹è¯•:")
    batch_inference(test_texts, labels=test_labels)

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_saved_model()
    
    # ç¤ºä¾‹ï¼šä½¿ç”¨ä¸åŒçš„æ¨¡å‹è·¯å¾„
    # inference("Your text here", model_path='saved_model/final_model')
    # inference("Your text here", model_path='saved_model/inference_model')
