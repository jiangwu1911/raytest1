# -*- coding: utf-8 -*-
import ray
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR
import time
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score, roc_curve, auc
import warnings
import os
warnings.filterwarnings("ignore")

# åˆå§‹åŒ– Ray
ray.init(f"ray://192.168.1.217:10001")  # è‡ªåŠ¨è¿æ¥åˆ° Ray é›†ç¾¤

print("Ray é›†ç¾¤ä¿¡æ¯:")
print(f"å¯ç”¨èŠ‚ç‚¹: {ray.available_resources()}")

# æ£€æŸ¥è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# æ•°æ®é¢„å¤„ç†
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# å®šä¹‰æ¨¡å‹ç±» - ä¿®å¤ torchvision è­¦å‘Š
def get_resnet18(num_classes=10):
    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

# ä½¿ç”¨ Ray çš„è¿œç¨‹å‡½æ•°è¿›è¡Œæ•°æ®åŠ è½½å’Œé¢„å¤„ç†
@ray.remote
def load_datasets():
    """åœ¨è¿œç¨‹èŠ‚ç‚¹ä¸ŠåŠ è½½æ•°æ®é›†"""
    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)
    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)
    return train_set, test_set

# ä½¿ç”¨ Ray çš„ Actor ç±»è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
@ray.remote(num_gpus=0.5)  # æ¯ä¸ª Actor ä½¿ç”¨ 1 ä¸ª GPU
class TrainingWorker:
    def __init__(self, worker_id, num_workers):
        self.worker_id = worker_id
        self.num_workers = num_workers
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = get_resnet18(num_classes=10)
        self.model = self.model.to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        self.scheduler = StepLR(self.optimizer, step_size=30, gamma=0.1)
        
        print(f"Worker {worker_id} åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def train_epoch(self, data_shard, epoch):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(data_shard, batch_size=128, shuffle=True)
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_loss = running_loss / len(train_loader)
        train_acc = 100. * correct / total
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler.step()
        
        return {
            'worker_id': self.worker_id,
            'epoch': epoch,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'learning_rate': self.scheduler.get_last_lr()[0]
        }

    def get_model_weights(self):
        """è·å–æ¨¡å‹æƒé‡"""
        return self.model.state_dict()

    def set_model_weights(self, weights):
        """è®¾ç½®æ¨¡å‹æƒé‡"""
        self.model.load_state_dict(weights)

    def validate(self, test_loader):
        """åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []
        all_outputs = []
        
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)

                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(targets.cpu().numpy())
                all_outputs.extend(outputs.cpu().numpy())

        test_loss = running_loss / len(test_loader)
        test_acc = 100. * correct / total
        
        return {
            'test_loss': test_loss,
            'test_acc': test_acc,
            'all_preds': all_preds,
            'all_labels': all_labels,
            'all_outputs': all_outputs
        }

# ä¿®å¤çš„è”é‚¦å¹³å‡ç®—æ³•
def federated_averaging(worker_weights):
    """å¯¹å¤šä¸ªworkerçš„æƒé‡è¿›è¡Œå¹³å‡"""
    averaged_weights = {}
    
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰æƒé‡çš„é”®
    all_keys = set()
    for weights in worker_weights:
        all_keys.update(weights.keys())
    
    # å¯¹æ¯ä¸ªé”®è¿›è¡Œå¹³å‡
    for key in all_keys:
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªworkerçš„è¯¥é”®çš„æ•°æ®ç±»å‹
        first_weight = worker_weights[0][key]
        
        if first_weight.dtype in [torch.int64, torch.int32, torch.long]:
            # å¯¹äºæ•´æ•°ç±»å‹çš„æƒé‡ï¼Œæˆ‘ä»¬ç›´æ¥å¤åˆ¶ç¬¬ä¸€ä¸ªworkerçš„å€¼
            # å› ä¸ºè¿™äº›é€šå¸¸æ˜¯bufferï¼ˆå¦‚num_batches_trackedï¼‰ï¼Œä¸éœ€è¦å¹³å‡
            averaged_weights[key] = first_weight.clone()
        else:
            # å¯¹äºæµ®ç‚¹æ•°ç±»å‹çš„æƒé‡ï¼Œè¿›è¡Œå¹³å‡
            averaged_weights[key] = torch.zeros_like(first_weight)
            for weights in worker_weights:
                averaged_weights[key] += weights[key]
            averaged_weights[key] /= len(worker_weights)
    
    return averaged_weights

# ç®€åŒ–çš„æƒé‡å¹³å‡ï¼ˆåªå¹³å‡å¯è®­ç»ƒå‚æ•°ï¼‰
def simple_weight_average(worker_weights):
    """ç®€åŒ–çš„æƒé‡å¹³å‡ï¼Œåªå¤„ç†å¯è®­ç»ƒå‚æ•°"""
    averaged_weights = {}
    
    # åªå¤„ç†ç¬¬ä¸€ä¸ªworkerçš„æƒé‡
    for key in worker_weights[0].keys():
        weight = worker_weights[0][key]
        
        # è·³è¿‡æ•´æ•°ç±»å‹çš„buffer
        if weight.dtype in [torch.int64, torch.int32, torch.long]:
            averaged_weights[key] = weight.clone()
            continue
            
        # å¯¹æµ®ç‚¹æ•°æƒé‡è¿›è¡Œå¹³å‡
        averaged_weights[key] = torch.zeros_like(weight)
        for weights in worker_weights:
            averaged_weights[key] += weights[key]
        averaged_weights[key] /= len(worker_weights)
    
    return averaged_weights

# å¯è§†åŒ–å‡½æ•°
def plot_confusion_matrix(all_labels, all_preds, classes):
    cm = confusion_matrix(all_labels, all_preds)
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix - Distributed Training', fontsize=16, fontweight='bold')
    plt.ylabel('Actual Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('ray_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Confusion matrix saved as 'ray_confusion_matrix.png'")

def plot_roc_curve(all_labels, all_outputs, classes):
    all_outputs = np.array(all_outputs)
    all_labels = np.array(all_labels)
    
    one_hot_labels = np.eye(len(classes))[all_labels]
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    plt.figure(figsize=(12, 10))
    colors = plt.cm.Set3(np.linspace(0, 1, len(classes)))
    
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(one_hot_labels[:, i], all_outputs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,
                label=f'{classes[i]} (AUC = {roc_auc[i]:.3f})')
    
    plt.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.6, label='Random Classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Multi-class ROC Curves - Distributed Training', fontsize=16, fontweight='bold')
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('ray_roc_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… ROC curve saved as 'ray_roc_curve.png'")
    
    macro_auc = roc_auc_score(one_hot_labels, all_outputs, multi_class='ovr', average='macro')
    return macro_auc

# ä¸»å‡½æ•°
def main():
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    
    print("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ...")
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¦ åŠ è½½æ•°æ®é›†...")
    train_set, test_set = ray.get(load_datasets.remote())
    test_loader = DataLoader(test_set, batch_size=100, shuffle=False)
    
    # é…ç½®è®­ç»ƒå‚æ•°
    num_workers = 2  # å¯ä»¥æ ¹æ®é›†ç¾¤GPUæ•°é‡è°ƒæ•´
    num_epochs = 50
    data_per_worker = len(train_set) // num_workers
    
    print(f"ä½¿ç”¨ {num_workers} ä¸ª workers")
    print(f"æ¯ä¸ª worker å¤„ç† {data_per_worker} ä¸ªæ ·æœ¬")
    
    # åˆ›å»º workers
    print("ğŸ‘¥ åˆ›å»ºè®­ç»ƒ workers...")
    workers = [TrainingWorker.remote(i, num_workers) for i in range(num_workers)]
    
    # æ•°æ®åˆ†ç‰‡
    data_shards = []
    for i in range(num_workers):
        start_idx = i * data_per_worker
        end_idx = start_idx + data_per_worker if i < num_workers - 1 else len(train_set)
        data_shard = torch.utils.data.Subset(train_set, range(start_idx, end_idx))
        data_shards.append(data_shard)
    
    # è®­ç»ƒå¾ªç¯
    best_acc = 0
    training_history = []
    
    print("ğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    start_time = time.time()
    
    for epoch in range(1, num_epochs + 1):
        print(f"\nğŸ“ Epoch {epoch}/{num_epochs}")
        
        # å¹¶è¡Œè®­ç»ƒæ‰€æœ‰ workers
        futures = []
        for i, worker in enumerate(workers):
            future = worker.train_epoch.remote(data_shards[i], epoch)
            futures.append(future)
        
        # æ”¶é›†è®­ç»ƒç»“æœ
        results = ray.get(futures)
        
        # æ‰“å°æ¯ä¸ª worker çš„ç»“æœ
        for result in results:
            print(f"Worker {result['worker_id']}: Loss={result['train_loss']:.3f}, Acc={result['train_acc']:.2f}%, LR={result['learning_rate']:.6f}")
        
        # è”é‚¦å¹³å‡ï¼šèšåˆæ¨¡å‹æƒé‡ï¼ˆæ¯5ä¸ªepochèšåˆä¸€æ¬¡ï¼‰
        if epoch % 5 == 0 or epoch == num_epochs:
            print("ğŸ”„ èšåˆæ¨¡å‹æƒé‡...")
            weight_futures = [worker.get_model_weights.remote() for worker in workers]
            worker_weights = ray.get(weight_futures)
            
            # ä½¿ç”¨ä¿®å¤çš„æƒé‡å¹³å‡å‡½æ•°
            averaged_weights = simple_weight_average(worker_weights)
            
            # åˆ†å‘å¹³å‡æƒé‡ç»™æ‰€æœ‰ workers
            set_weight_futures = [worker.set_model_weights.remote(averaged_weights) for worker in workers]
            ray.get(set_weight_futures)
            print("âœ… æ¨¡å‹æƒé‡èšåˆå®Œæˆ")
        
        # éªŒè¯ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªworkerï¼‰
        if epoch % 10 == 0 or epoch == num_epochs:
            print("ğŸ§ª æ¨¡å‹éªŒè¯...")
            validation_result = ray.get(workers[0].validate.remote(test_loader))
            test_acc = validation_result['test_acc']
            
            print(f"ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {test_acc:.2f}%")
            
            if test_acc > best_acc:
                best_acc = test_acc
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_weights = ray.get(workers[0].get_model_weights.remote())
                torch.save(best_weights, 'ray_best_model.pth')
                print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼Œå‡†ç¡®ç‡: {best_acc:.2f}%")
            
            training_history.append({
                'epoch': epoch,
                'test_acc': test_acc,
                'worker_results': results
            })
    
    end_time = time.time()
    print(f'\nâœ… è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {(end_time - start_time) // 60:.0f}åˆ† {(end_time - start_time) % 60:.0f}ç§’')
    print(f'ğŸ¯ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%')
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_weights = torch.load('ray_best_model.pth')
    ray.get(workers[0].set_model_weights.remote(best_weights))
    
    # æœ€ç»ˆéªŒè¯
    final_result = ray.get(workers[0].validate.remote(test_loader))
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix(final_result['all_labels'], final_result['all_preds'], classes)
    
    print("ğŸ“ˆ ç”ŸæˆROCæ›²çº¿...")
    macro_auc = plot_roc_curve(final_result['all_labels'], final_result['all_outputs'], classes)
    
    # è®¡ç®—æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(
        final_result['all_labels'], final_result['all_preds'], average='weighted'
    )
    
    final_accuracy = 100 * np.sum(np.array(final_result['all_preds']) == np.array(final_result['all_labels'])) / len(final_result['all_labels'])
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.2f}%")
    print(f"ğŸ“Š åŠ æƒç²¾ç¡®ç‡: {precision:.4f}")
    print(f"ğŸ“Š åŠ æƒå¬å›ç‡: {recall:.4f}")
    print(f"ğŸ“Š åŠ æƒF1åˆ†æ•°: {f1:.4f}")
    print(f"ğŸ“Š å®è§‚å¹³å‡AUC: {macro_auc:.4f}")
    
    # æ¸…ç† Ray
    ray.shutdown()
    
    print("\nâœ… åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆ!")
    print("   - ray_confusion_matrix.png: æ··æ·†çŸ©é˜µ")
    print("   - ray_roc_curve.png: ROCæ›²çº¿") 
    print("   - ray_best_model.pth: æœ€ä½³æ¨¡å‹æƒé‡")

if __name__ == "__main__":
    main()
